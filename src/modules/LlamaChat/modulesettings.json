{
  "Modules": {

    "LlamaChat": {
      "Name": "LlamaChat",
      "Version": "1.1.2",

      // Publishing info
      "Description": "A Large Language Model based on the Machine Learning Compilation for LLMs",
      "Platforms": [ ],
      "Category": "Generative AI",
      "License": "MIT",
      "LicenseUrl": "https://github.com/abetlen/llama-cpp-python/blob/main/LICENSE.md",

      // Launch instructions
      "AutoStart": false,
      "FilePath": "Llama_adapter.py",
      "Runtime": "python3.8",
      "RuntimeLocation": "Local",       // Can be Local or Shared

      "EnvironmentVariables": {
        "CPAI_LOG_VERBOSITY": "Info"
      },

      // GPU options
      "InstallGPU": true,               // GPU support not provided
      "EnableGPU": true,                // Will be coerced to false if InstallGPU = false
      "AcceleratorDeviceName": null,    // = default
      "Parallelism": 1,                 // 0 = Default (number of CPUs - 1)
      "HalfPrecision": "enable",        // 'Force', 'Enable', 'Disable': whether to force on, allow, or disable half-precision ops
      "PostStartPauseSecs": 0,          // Generally 1 if using GPU, 0 for CPU

      // Which server version is compatible with each version of this module.
      "ModuleReleases": [
        { "ModuleVersion": "1.0",   "ServerVersionRange": [ "2.3.1", "2.3.5" ], "ReleaseDate": "2022-11-01" },
        { "ModuleVersion": "1.1",   "ServerVersionRange": [ "2.3.5", "2.4.0" ], "ReleaseDate": "2022-11-10", "ReleaseNotes": "Updated to match new installer SDK." },
        { "ModuleVersion": "1.1.1", "ServerVersionRange": [ "2.4.1", "2.4.1" ], "ReleaseDate": "2023-12-06", "ReleaseNotes": "Updated modulesettings schema", "Importance": "Minor" },
        { "ModuleVersion": "1.1.2", "ServerVersionRange": [ "2.4.2", ""      ], "ReleaseDate": "2023-12-09", "ReleaseNotes": "Installer updates", "Importance": "Minor" }
      ],

      "RouteMaps": [
        {
          "Name": "LlamaChat",
          "Route": "text/chat",
          "Method": "POST",
          "Command": "prompt",
          "MeshEnabled": false,
          "Description": "Uses the Llama LLM to answer simple wiki-based questions.",
          "Inputs": [
            {
              "Name": "prompt",
              "Type": "Text",
              "Description": "The prompt to send to the LLM"
            }
          ],
          "Outputs": [
            {
              "Name": "success",
              "Type": "Boolean",
              "Description": "True if successful."
            },
            {
              "Name": "reply",
              "Type": "Text",
              "Description": "The reply from the model."
            },
            {
              "Name": "command",
              "Type": "String",
              "Description": "The command that was sent as part of this request. Can be detect, list, status."
            },
            {
              "Name": "moduleId",
              "Type": "String",
              "Description": "The Id of the module that processed this request."
            },
            {
              "Name": "executionProvider",
              "Type": "String",
              "Description": "The name of the device or package handling the inference. eg CPU, GPU, TPU, DirectML."
            },
            {
              "Name": "canUseGPU",
              "Type": "Boolean",
              "Description": "True if this module can use the current GPU if one is present."
            },
            {
              "Name": "inferenceMs",
              "Type": "Integer",
              "Description": "The time (ms) to perform the AI inference."
            },
            {
              "Name": "processMs",
              "Type": "Integer",
              "Description": "The time (ms) to process the image (includes inference and image manipulation operations)."
            },
            {
              "Name": "analysisRoundTripMs",
              "Type": "Integer",
              "Description": "The time (ms) for the round trip to the analysis module and back."
            }
          ]
        }
      ]
    }
  }
}
