{
  "Modules": {

    "LlamaChat": {
      "Name": "LlamaChat",
      "Version": "1.3.0",

      "PublishingInfo" : {
        "Description": "A Large Language Model based on the Machine Learning Compilation for LLMs",
        "IconURL": null,
        "Category": "Generative AI",
        "Stack": "Python, Llama",
        "License": "MIT",
        "LicenseUrl": "https://github.com/abetlen/llama-cpp-python/blob/main/LICENSE.md",
        "Author": "Chris Maunder",
        "Homepage": "https://codeproject.com/ai",
        "BasedOn": "llama-cpp-python",
        "BasedOnUrl": "https://github.com/abetlen/llama-cpp-python"
      },

      "LaunchSettings": {
        "AutoStart": false,
        "FilePath": "Llama_adapter.py",
        "Runtime": "python3.8",
        "RuntimeLocation": "Local",       // Can be Local or Shared
        "PostStartPauseSecs": 0,          // Generally 1 if using GPU, 0 for CPU
        "Queue": null                     // Use default
      },

      "EnvironmentVariables": {
        "CPAI_LOG_VERBOSITY": "Quiet"
      },

      "GpuOptions" : {
        "InstallGPU": true,               // GPU support not provided
        "EnableGPU": true,                // Will be coerced to false if InstallGPU = false
        "AcceleratorDeviceName": null,    // = default
        "Parallelism": 1,                 // 0 = Default (number of CPUs - 1)
        "HalfPrecision": "enable"         // 'Force', 'Enable', 'Disable': whether to force on, allow, or disable half-precision ops
      },
      
      "InstallOptions" : {
        "Platforms":      [  ],  // DISABLED FOR NOW. The platforms this module can and cannot be installed on
        "PreInstalled":   false,          // Is this module pre-installed with the server (eg Docker containers)
        "ModuleReleases": [               // Which server version is compatible with each version of this module.
          { "ModuleVersion": "1.0",   "ServerVersionRange": [ "2.3.1", "2.3.5" ], "ReleaseDate": "2022-11-01" },
          { "ModuleVersion": "1.1",   "ServerVersionRange": [ "2.3.5", "2.4.0" ], "ReleaseDate": "2022-11-10", "ReleaseNotes": "Updated to match new installer SDK." },
          { "ModuleVersion": "1.1.1", "ServerVersionRange": [ "2.4.1", "2.4.1" ], "ReleaseDate": "2023-12-06", "ReleaseNotes": "Updated modulesettings schema", "Importance": "Minor" },
          { "ModuleVersion": "1.1.2", "ServerVersionRange": [ "2.4.2", "2.4.9" ], "ReleaseDate": "2023-12-09", "ReleaseNotes": "Installer updates", "Importance": "Minor" },
          { "ModuleVersion": "1.2.0", "ServerVersionRange": [ "2.5.0-RC1", "2.5.0-RC5" ], "ReleaseDate": "2024-01-06", "ReleaseNotes": "Additions for dynamic explorer UI" },
          { "ModuleVersion": "1.2.1", "ServerVersionRange": [ "2.5.0-RC1", "2.5.0-RC5" ], "ReleaseDate": "2024-01-13", "ReleaseNotes": "Changes to SDK" },
          { "ModuleVersion": "1.2.2", "ServerVersionRange": [ "2.5.0-RC6", "" ], "ReleaseDate": "2024-01-16", "ReleaseNotes": "Updated modulesettings schema" },
          { "ModuleVersion": "1.2.3", "ServerVersionRange": [ "2.5.0-RC6", "" ], "ReleaseDate": "2024-01-18", "ReleaseNotes": "Updated explorer" },
          { "ModuleVersion": "1.3.0", "ServerVersionRange": [ "2.5.0-RC6", "" ], "ReleaseDate": "2024-01-21", "ReleaseNotes": "Module performance statistics added" }
        ]
      },

      "RouteMaps": [
        {
          "Name": "LlamaChat",
          "Route": "text/chat",
          "Method": "POST",
          "Command": "prompt",
          "MeshEnabled": false,
          "Description": "Uses the Llama LLM to answer simple wiki-based questions.",
          "Inputs": [
            {
              "Name": "prompt",
              "Type": "Text",
              "Description": "The prompt to send to the LLM"
            }
          ],
          "Outputs": [
            {
              "Name": "success",
              "Type": "Boolean",
              "Description": "True if successful."
            },
            {
              "Name": "reply",
              "Type": "Text",
              "Description": "The reply from the model."
            },
            {
              "Name": "inferenceMs",
              "Type": "Integer",
              "Description": "The time (ms) to perform the AI inference."
            },
            {
              "Name": "processMs",
              "Type": "Integer",
              "Description": "The time (ms) to process the image (includes inference and image manipulation operations)."
            }
          ]
        }
      ]
    }
  }
}
